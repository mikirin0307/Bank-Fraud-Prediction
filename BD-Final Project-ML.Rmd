---
title: "BD-Final Project-EDA"
output: html_document
date: "2023-05-11"
---
```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(clustMixType)
```

```{r}
library(rpart) 
library(xgboost) 
library(ranger)
library(caret)
```

#Data Wrangling

```{r}
rm(list=ls())
setwd("/Users/mikirin/Documents/GitHub/Bank-Fraud-Prediction")

fraud_data <- read.csv('Base.csv')

#check NAs
sapply(fraud_data, function(x) sum(is.na(x)))

#check the data type of each variable
#str(fraud_data)

#turn numeric variables with less than 9 unique values into categorical variables 
unique.per.column <- sapply(select_if(fraud_data, is.numeric), n_distinct)
column.names.to.factor <- names(unique.per.column)[unique.per.column < 9]
fraud_data <- mutate_at(fraud_data, column.names.to.factor, as.factor)

#delete column where all value = 0
str(fraud_data)
fraud_data <- fraud_data %>% select(-device_fraud_count)

```

sloving the imbalance issues (from about 100,000 to 20,000 records)
```{r}

set.seed(1)
#first, there are imbalances in our y_value. We use downsampling to make our imbalanced data balanced. This could improve our prediction. 

#Undersampling for Unbalanced Datasets
fraud_data = downSample(x = select(fraud_data, -fraud_bool),
                  y = fraud_data$fraud_bool,
                  list = FALSE, yname = "fraud_bool")
table(fraud_data$fraud_bool)

str(fraud_data)
#second, there are outliers in X values, according to our EDA. Only 1 BG, and 4 AE. it wouldn't affect our prediction. So delete them. 
table(fraud_data$housing_status)
table(fraud_data$payment_type)

fraud_data <- fraud_data[fraud_data$housing_status != 'BG', ]
fraud_data <- fraud_data[fraud_data$payment_type != 'AE', ]

```

##Clustering 
K-means / Hierarchical clustering are not appropriate because we have both categorical and numerical variables. Thus, I choose K-Prototype algorithm.  Model explain (K-Prototype Clustering)

```{r}
# identify numeric and factor columns
is_num_or_factor <- sapply(fraud_data, function(x) is.numeric(x) | is.factor(x))

# subset the data to include only these columns
fraud_data_num_and_factor <- fraud_data[, is_num_or_factor]
fraud_data_num_and_factor <- fraud_data_num_and_factor %>% select(-fraud_bool)

credit_clusters <- kproto(fraud_data_num_and_factor, 3)
credit_clusters
summary(credit_clusters)

# save the cluster assignments to our data
fraud_data$cluster <- credit_clusters$cluster

# examine 
table(fraud_data$cluster)

```
**Methodology**   
We performed a cluster analysis on the bank fraud data using the k-prototypes method. The resulting clusters provide groupings of the data based on similarity across the variables included in the dataset.The k-prototypes algorithm, a combination of k-means and k-modes, works particularly well in the dataset as a mixture of categorical and numerical features.   
**Observations**
Cluster 1:   
This cluster is characterized by a lower 'velocity_24h' and 'velocity_4w', suggesting a relatively lower transaction frequency within those periods. The higher credit risk score for this group compared to the other clusters indicates that these customers are generally considered less risky according to traditional credit scoring models. Predominantly, these customers use free email services, as indicated by the 'email_is_free' variable being 1. Furthermore, they tend not to possess other cards, as evidenced by 'has_other_cards' being 0, and they typically do not make foreign requests. Notably, customers in this group tend to have longer session lengths, suggesting they spend more time on the platform during a single session.

Cluster 2:
Customers in this cluster display moderate transaction activity, as evidenced by the second highest velocity variables ('velocity_24h' and 'velocity_4w'). Interestingly, this cluster also exhibits the highest credit risk score, suggesting these customers are perceived as the least risky group. Similar to Cluster 1, customers here also do not tend to have other cards or make foreign requests. However, their session lengths are the shortest among the clusters, indicating they spend less time on the platform per session.

Cluster 3:
The third cluster comprises customers with the highest transaction frequency in the 'velocity_24h' and 'velocity_4w' periods. These customers have the lowest credit risk scores among the clusters, suggesting they are the riskiest group. Most customers in this cluster also use free email services. As with the other clusters, they typically do not possess other cards or make foreign requests. The session lengths for customers in this cluster fall in between those of Clusters 1 and 2.

These insights indicate that transaction velocity measures ('velocity_24h', 'velocity_4w', 'velocity_6h') and 'credit_risk_score' are substantial differentiators between the clusters. It's also clear that certain categorical variables such as 'email_is_free', 'has_other_cards', and 'foreign_request' contribute to distinguishing these groups.

However, certain variables, including 'income', 'customer_age', among others, show less variation across clusters. This could suggest their limited utility in differentiating types of credit fraud or imply a consistent influence across different fraud types.The categorical attributes including 'email_is_free', 'phone_home_valid', and others, showed some variation across the clusters, suggesting their potential role in distinguishing between different types of credit fraud. Thus, we would supplement these findings in clustering with additional analyses and predictive models for a comprehensive understanding of credit fraud patterns.




#Machine Learning
```{r}
#partition the data into 60% training and 40% validation set

set.seed(123)

fraud_data$cluster <- as.factor(fraud_data$cluster)
n = dim(fraud_data)[1]
ind = sample(n, floor(0.6*n))
fraud.train = fraud_data[ind,]
fraud.test = fraud_data[-ind,]

paste("Number of observations in the training set:", dim(fraud.train)[1])
paste("Number of observations in the validation set:", dim(fraud.test)[1])


```

#Some helper functions:
```{r}
get_deviance = function(y, phat, wht=1e-7){
  if(is.factor(y)) y = as.numeric(y)-1
  phat = (1-wht)*phat + wht*.5
  py = ifelse(y==1, phat, 1-phat)
  return(-2*sum(log(py)))
}

get_confusion_matrix = function(y, phat, thr=0.5){
  if(is.numeric(y)) y = as.factor(y)
  yhat = as.factor(ifelse(phat > thr, 1, 0)) 
  confusionMatrix(yhat, y)
}
```

where phats will be stored:
```{r}
phat_list = list() 
```


#Logistic Regression:

```{r}
set.seed(123)

logistic <- glm(fraud_bool ~ ., family = binomial, data = fraud.train)

summary(logistic)

#prediction based on logistic regression
logit.phat = predict(logistic, fraud.test, type = 'response')

#store all predictions into a list
phat_list$logit.phat = matrix(logit.phat,ncol=1)


```
**Model Specification:** We applied a logistic regression model to predict the probability of fraudulent activities using the provided predictors in the dataset. The target variable was a boolean 'fraud_bool', denoting the occurrence of fraud. The regression results and key insights are detailed below.

**Model Output:** Our logistic regression model (glm function with a binomial family) had a residual deviance of 11634 on 13175 degrees of freedom, suggesting a decent fit of the model to the data. This is an improvement over the null deviance, which stood at 18341 on 13229 degrees of freedom.   
Several predictors turned out to be highly statistically significant, as suggested by their p-values (p < 0.05), including essential income indicators such as income and employment status ('income', 'employment_statusCB', 'employment_statusCD', 'employment_statusCE', 'employment_statusCF'), those indicating housing status, etc.. Meanwhile, there are a few variables less significant, such as payment type and 'velocity_6h', 'velocity_24h', 'velocity_4w'(p > 0.05).    

An interesting observation in our regression analysis pertains to the variables 'velocity_6h', 'velocity_24h', and 'velocity_4w'. Despite their variance being pronounced in the clusters, they are not statistically significant in the logistic regression model. This suggests that while these variables might show significant differences across different clusters, they do not contribute meaningfully to the prediction of fraud in the model.

This apparent contradiction may arise from various reasons. One explanation could be the presence of collinearity - these velocity variables might be highly correlated with other predictors in the model, leading to their significance being 'masked'. Alternatively, the clustering algorithm may be detecting patterns and structures in these variables that the logistic regression model isn't capturing due to its assumptions and design.


Confusion matrix and accuracy:
```{r}
#confusion matrix
get_confusion_matrix(as.factor(fraud.test$fraud_bool), logit.phat)
```

```{r}
3500/(3500+904)
```
#precision for identifying fraud case: 0.7947321

**Model Performance:** The model achieved an overall accuracy of approximately 80%, as per the confusion matrix. The sensitivity and specificity were also in the same range. This result suggests that the model has a good balance between correctly identifying positive instances (fraud) and negative instances (non-fraud).


#Knn: 
```{r}
library(kknn)

# Feature scaling for training and testing data
fraud.train.knn <- data.frame(model.matrix(~.-1, fraud.train)) 
fraud.test.knn <- data.frame(model.matrix(~.-1, fraud.test))

# Feature scaling for training and testing data
fraud.train.scaled <- data.frame(scale(fraud.train.knn[,-31]))
fraud.test.scaled <- data.frame(scale(fraud.test.knn[,-31]))

# Add the response variable back to the scaled data frames
fraud.train.scaled$fraud_bool <- fraud.train$fraud_bool
fraud.test.scaled$fraud_bool <- fraud.test$fraud_bool

table(fraud.train$payment_type)
```

```{r}

set.seed(123)

# Define the grid for k and distance parameters
grid_knn = expand.grid(
  k = c(5, 10),  # Number of neighbors
  distance = c(1, 2)   # Distance metric
)

# Initialize a matrix to store probabilities
phat_list$knn = matrix(0, nrow(fraud.test), nrow(grid_knn))

# Loop over the grid
for (i in 1:nrow(grid_knn)){
  # Train the k-NN model
  knn_fit = train.kknn(
    formula = fraud_bool~., 
    data = fraud.train.scaled, 
    kmax = grid_knn$k[i],
    distance = grid_knn$distance[i], 
    kernel = "rectangular", 
    scale = TRUE
  )

  # Store the predicted probabilities
  phat_list$knn[,i] = predict(knn_fit, newdata = fraud.test.scaled, type = "prob")[,2]
}

```

Grid Search for Knn
```{r}
# Calculate losses for each parameter combination
losses_knn = c()
for (i in 1:nrow(grid_knn)){
  losses_knn[i] = get_deviance(fraud.test$fraud_bool, phat_list$knn[,i])
}

# Get the best parameters
best_param_knn = grid_knn[which.min(losses_knn),]
best_param_knn
# Train the k-NN model with the best parameters
best_knn_fit = train.kknn(
  formula = fraud_bool~., 
  data = fraud.train.scaled, 
  kmax = 10,
  distance = 1, 
  kernel = "rectangular", 
  scale = TRUE
)

# Generate predictions using the best model
best_phat_knn = predict(best_knn_fit, newdata = fraud.test.scaled, type = "prob")[,2]

```
**Model Specification:**    
We applied k-NN model which assumes that data points closer in proximity are more similar and thus share the same class.   
We first scaled the training and testing data as a preprocessing step for the k-NN algorithm, as it operates based on the distances between data points. By doing so, we prevented the algorithm from being unduly influenced by features with larger scales. We then conducted a grid search over two key hyperparameters of the k-NN model: 'k' (the number of nearest neighbors considered) and 'distance' (which dictates the type of distance metric used). This process involved training the k-NN model on combinations of 'k' values of 5 and 10, and 'distance' values of 1 and 2. The optimal combination of these hyperparameters was determined by identifying the set of parameters that minimized the deviance.   

**Model Output:**    
According to the results, the best hyperparameters for this model were found to be 'k' equals to 10 and 'distance' equals to 1. The final k-NN model was therefore trained using these optimal parameters.

Confusion matrix by KNN
```{r}
knn_res = get_confusion_matrix(fraud.test$fraud_bool, best_phat_knn)
knn_res$table

# Print the accuracy 
paste('Accuracy:', round(knn_res$overall[1],3))

```

**Model Performance:*   
The confusion matrix showed that our model achieved an overall accuracy of approximately 93.6%, highlighting its proficiency at correctly identifying both fraudulent and non-fraudulent transactions. Precision, the proportion of positive identifications that were indeed correct, was calculated to be approximately 0.933, indicating a relatively low false positive rate of our model. On the other hand, recall, the proportion of actual positives that were identified correctly, came out to be about 0.938. This metric is particularly significant in our fraud detection scenarios as it represents the model's capability to correctly identify fraudulent transactions out of all actual fraudulent cases. A higher Recall means that the model misses fewer fraudulent transactions. The above statistical features suggest encouraging results that our k-NN model with k=10 and distance=1 has demonstrated a strong performance in the test data.




#Decision Tree
Fit trees:
```{r}
grid_tree = expand.grid(
  max_depth = c(5, 10, 20),
  cp = c(0.1, 0.01, 0.001),
  minsplit = c(1, 5, 10, 20),
  minbucket = c(5, 10, 15)
)

phat_list$tree = matrix(0, nrow(fraud.test), nrow(grid_tree))

for (i in 1:nrow(grid_tree)){
  tree_fit = rpart(
    formula = fraud_bool~., 
    data = fraud.train,
    method = 'class',
    control = rpart.control(
      max_depth = grid_tree$max_depth[i],
      cp = grid_tree$cp[i],
      minsplit = grid_tree$minsplit[i],
      min_bucket = grid_tree$minbucket[i])
  )
    phat_list$tree[,i] = predict(tree_fit, newdata=fraud.test[,-31], type="prob")[,2]
}


```

Tree with best parameters:
```{r}
losses_tree = c()
for (i in 1:nrow(grid_tree)){
  losses_tree[i] = get_deviance(fraud.test$fraud_bool, phat_list$tree[,i])
}

best_param_tree = grid_tree[which.min(losses_tree),]

best_tree_fit = rpart(
  formula = fraud_bool~.,
  data = fraud.train,
  method = 'class',
  control = rpart.control(
    cp = best_param_tree$cp,
    minsplit = best_param_tree$minsplit)
  )

best_phat_tree = predict(best_tree_fit, newdata=fraud.test[,-31], type='prob')[,2]
```
**Model Specification:**   
In this analysis, we explored a Decision Tree model to classify fraudulent transactions.
The model was trained using a range of hyperparameters which were defined in the grid search:   

Maximum depth: 5, 10, 20   
Complexity parameter (cp): 0.1, 0.01, 0.001   
Minimum split: 1, 5, 10, 20   
Minimum bucket: 5, 10, 15   
Each parameter combination was run through the model, with the resulting probabilities stored. Following this, the model's performance was evaluated using the deviance between the test data and the predicted probabilities. The best parameter combination was identified as having the smallest deviance, and the model was then retrained using these optimal parameters.




Confusion Matrix & Accuracy:
```{r}
tree_res = get_confusion_matrix(fraud.test$fraud_bool, best_phat_tree)
tree_res$table

#accuracy 
paste('Accuracy:', round(tree_res$overall[1],3))

#0.772 accuracy 

#Precision
#For Fraud:
3400/(3400+1038)
#Precision (Fraud) = TP / (TP + FP):0.7661109

#For No Fraud:
3409 / (3409 + 977)
#Precision (No Fraud) = TN / (TN + FN) = 0.7772458

```
**Model Performance:*   
The model achieved an accuracy of approximately 76.7%. This value is lower than what was observed in the k-NN model, suggesting that it might not be as proficient at correctly identifying fraudulent and non-fraudulent transactions. Since the model is used for bank fraud prediction, reducing false negatives - cases where a fraudulent transaction is missed, is crucial. The decision tree model appears to have a higher rate of false negatives, which could be a drawback in real-world scenarios.Thus, while decision tree models are interpretable and easy to visualize, the performance of this particular model suggests that more work may be needed to refine and improve it. We will proceed to explore more advanced tree-based models like Gradient Boosted Machines and Random Forests.




#Boosting - XGBoost Model

Preprocessing:
```{r}
library(dbarts)
fraud.train[,-31]
X_train = makeModelMatrixFromDataFrame(fraud.train[,-31])
y_train = as.numeric(fraud.train$fraud_bool)-1
X_val = makeModelMatrixFromDataFrame(fraud.test[,-31])
y_val = as.numeric(fraud.test$fraud_bool)-1

```


Fit boosted trees:
```{r}
grid_boost = expand.grid(
  shrinkage = c(0.1, 0.01, 0.001, 0.0001), 
  interaction.depth = c(1, 2, 4),
  nrounds = c(1000, 2000, 5000)
)

phat_list$boost = matrix(0, nrow(fraud.test), nrow(grid_boost)) 

for (i in 1:nrow(grid_boost)){
  params = list(
    eta = grid_boost$shrinkage[i], 
    max_depth = grid_boost$interaction.depth[i]
  )
  
  set.seed(4776)
  
  boost_fit = xgboost(
    data = X_train,
    label = y_train,
    params = params,
    nrounds = grid_boost$nrounds[i],
    objective = 'binary:logistic',
    verbose = 0,
    verbosity = 0
  )
  
  phat_list$boost[,i] = predict(boost_fit, newdata=X_val)
}

boost_fit
```
Boosted trees with best parameters:

```{r}
losses_boost = c()
for (i in 1:nrow(grid_boost)){
  losses_boost[i] = get_deviance(y_val, phat_list$boost[,i])
}

best_param_boost = grid_boost[which.min(losses_boost),]

best_param = list(
  eta = best_param_boost$shrinkage, 
  max_depth = best_param_boost$interaction.depth
)

set.seed(4776)

boost_fit_best = xgboost(
    data = X_train,
    label = y_train,
    params = best_param,
    nrounds = best_param_boost$nrounds,
    objective = 'binary:logistic',
    verbose = 0,
    verbosity = 0
  )

best_phat_boost = predict(boost_fit_best, newdata=X_val)
```

**Model Specification:**
Rather than just constructing a single decision tree, we use XGBoost to create a series of trees that successively learn from the errors of their predecessors.
We established a grid of hyperparameters, including the shrinkage parameter (eta), the interaction depth (max_depth), and the number of rounds (nrounds), which serves as a guide to explore different combinations of parameters, refining our model in the process. For each combination of hyperparameters, we trained an XGBoost model and made predictions on the test data. We then selected the model with the lowest deviance (i.e., the best fit) as our final model.



Confusion matrix & accuracy:
```{r}
boost_res = get_confusion_matrix(y_val, best_phat_boost)
boost_res$table

```
```{r}
paste('Accuracy:', round(boost_res$overall[1],3))

```

**Model Performance:**

The XGBoost model, in this scenario, performed considerably better than the individual decision tree model. The overall accuracy of the XGBoost model was approximately 0.818, compared to 0.767 for the decision tree model. Furthermore, the model achieved a recall of approximately 81.9%, meaning that it was able to detect about 81.9% of all actual fraudulent transactions. The improvement is likely due to the ability of the XGBoost model to sequentially correct the mistakes of previous trees, handle various types of predictor variables, manage missing values, and control overfitting.

precision: 3587/(3587+817) = 0.8144868

##Wrong!!
#Random Forest: There's some issues with RF method here. The accuracy is only 0.16.. My method may be flawed. please check!
```{r}
grid_rf = expand.grid(
  num_tree = c(200, 500, 1000,2000),
  sample_frac = seq(from=0.2, to=0.8, by=0.1),
  node_size = c(1, 5, 10,20, 30)
)

phat_list$rf = matrix(0, nrow(fraud.test), nrow(grid_rf))

for(i in 1:nrow(grid_rf)){
    rf_fit = ranger(
      formula = fraud_bool~.,
      data = fraud.train,
      num.trees = grid_rf$num_tree[i],
      mtry = 4, 
      sample.fraction = grid_rf$sample_frac[i],
      min.node.size = grid_rf$node_size[i], 
      probability = TRUE,
      seed = 1
    )
    
  phat_list$rf[,i] = predict(rf_fit, data=fraud.test[,-31])$predictions[,1]
}
```

random forests with best parameters:
```{r}
losses_rf = c()
for (i in 1:nrow(grid_rf)){
  losses_rf[i] = get_deviance(fraud.test$fraud_bool, phat_list$rf[,i])
}

best_param_rf = grid_rf[which.min(losses_rf),]

best_rf_fit = ranger(
    formula = fraud_bool~.,
    data = fraud.train,
    num.trees = best_param_rf$num_tree,
    mtry = 4, 
    sample.fraction = best_param_rf$sample_frac,
    min.node.size = best_param_rf$node_size, 
    probability = TRUE,
    seed = 1
)

best_phat_rf = predict(best_rf_fit, data=fraud.test[,-31])$predictions[,1]
```

Confusion matrix and accuracy:
```{r}
rf_res = get_confusion_matrix(fraud.test$fraud_bool, best_phat_rf)
rf_res$table

paste('Accuracy:', round(rf_res$overall[1],3))

```

##SVM (support vector machine)
```{r}
set.seed(4776)
svm <- caret::train(fraud_bool ~., 
                    data = fraud.train, 
                    method = "svmLinear3",
                    preProcess = c("scale", "center"),
                    trControl = trainControl(method = "repeatedcv",
                                             number = 10,
                                             repeats = 3,
                                             savePredictions = TRUE,
                                             verboseIter = FALSE))

plot(svm)

print(
paste(
"The final value used for the model was cost =",
svm[["finalModel"]][["tuneValue"]][["cost"]],
"and Loss =",
svm[["finalModel"]][["tuneValue"]][["Loss"]]
)
)

#predict
svm_predict = predict(svm, fraud.test)
#Confusion Matrix, o-o-s
svm_cm=confusionMatrix(svm_predict, fraud.test$fraud_bool)
svm_cm

#accuracy
accuracy_rate_svm =svm_cm$overall[["Accuracy"]]
paste('accuracy_rate_svm',accuracy_rate_svm)

#precision 
precision <- svm_cm$byClass["Pos Pred Value"]
paste('precision',precision)

phat_list$svm <- matrix(svm_predict , ncol = 1)

```
**Model Output:**
We use a linear SVM for a binary classification / prediction of bank fraud incidents. During the training of the SVM model, the chosen parameter values were cost = 0.5 and Loss = L2.
cost = 0.5: This is the cost parameter in SVM, which determines the penalty for misclassifications. A cost value of 0.5 indicates a moderate penalty for misclassifications.
Loss = L2: This refers to the loss function used in the SVM model. L2 loss, also known as the squared error loss, measures the extent of misclassifications using the Euclidean distance (squared error). Choosing L2 loss means that the model aims to minimize the squared error distance to minimize misclassifications.

**Model Performance:**
Our SVM model was feature by 0.7978 overall accuracy, 0.7954 precision (actual fraud cases that were correctly identified as fraud), and 0.8026 recall (proportion of actual fraud cases that were correctly identified). It performed slightly better than the Decision Tree and XGBoost models, but not as well as the KNN model. SVMs are effective in high-dimensional spaces and best suited for binary classification problems, but they can be sensitive to the choice of the kernel and the regularization parameter.

```{r}
set.seed(4776)
# Load the randomForest library
library(randomForest)

# Set mtry = 10 for faster execution
rf_model <- randomForest(fraud_bool ~ ., data = fraud.train, ntree = 100, mtry = 10)
rf_predictions <- predict(rf_model, newdata = fraud.test)

# Calculate accuracy
accuracy <- sum(rf_predictions == fraud.test$fraud_bool) / nrow(fraud.test)
paste('accuracy:', accuracy)

# Calculate precision
confusion_matrix <- table(rf_predictions, fraud.test$fraud_bool)
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
paste('precision:', precision)

# Display confusion matrix
print(confusion_matrix)

# Initialize variables to track best parameters
best_accuracy <- -1
best_params <- NULL

# Loop through different parameter combinations
for (mtry in c(5, 10, 15)) {
  for (ntree in c(50, 100, 150)) {
    # Build random forest model with current parameter combination
    rf_model <- randomForest(fraud_bool ~ ., data = fraud.train, ntree = ntree, mtry = mtry)
    rf_predictions <- predict(rf_model, newdata = fraud.test)
    
    # Calculate accuracy
    accuracy <- sum(rf_predictions == fraud.test$fraud_bool) / nrow(fraud.test)
    
    # Check if current accuracy is better than the best accuracy
    if (accuracy > best_accuracy) {
      best_accuracy <- accuracy
      best_params <- c(ntree, mtry)
    }
  }
}

# Print the best parameters and accuracy
paste('Best parameters:', best_params)
paste('Best accuracy:', best_accuracy)

```

**Model Output:**
The best parameters found for this model are:

The number of trees (ntree) is 150.
The number of variables randomly sampled at each split (mtry) is 5.

**Model Performance:**
The highest accuracy achieved with these parameters is approximately 0.8. The Random Forest model was also featured by a precision of 80.36%, and recall of 80.35%. We see that the Boosted model outperforms all others in accuracy, precision, and recall. But a Random Forest model can be more efficient in terms of the computational cost.
